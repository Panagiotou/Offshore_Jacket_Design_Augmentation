{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ON GPU\n"
     ]
    }
   ],
   "source": [
    "from be_great import GReaT\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils_methods import encode, get_var_metadata, post_process, transform_df_to_json, generate_plots, check_constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = ''\n",
    "res_folder = main_folder + \"results/LLM/\"\n",
    "data_folder = main_folder + \"data/\"  \n",
    "\n",
    "with open(data_folder + \"real_structures.json\") as f:\n",
    "    real_orig = json.load(f)\n",
    "DATA_ARRAY = [real_orig]\n",
    "DATA_NAMES = [\"real\"]\n",
    "\n",
    "USE_INFORMATIVE_NAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brace_dict = {    \n",
    "    \"NONE\": 0,\n",
    "    \"H\": 1,\n",
    "    \"Z\": 2,\n",
    "    \"IZ\": 3,\n",
    "    \"ZH\": 4,\n",
    "    \"IZH\": 5,\n",
    "    \"K\": 6,\n",
    "    \"X\": 7,\n",
    "    \"XH\": 8,\n",
    "    \"nan\": 9,\n",
    "}\n",
    "\n",
    "brace_dict_inv = dict(zip(brace_dict.values(), brace_dict.keys()))\n",
    "\n",
    "N_BRACES = len(brace_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = max(d.get('n_layers', 0) for sublist in DATA_ARRAY for d in sublist)\n",
    "transformed_columns = [\"legs\", \"total_height\", \"radius_bottom\", \"radius_top\", \"n_layers\"]\n",
    "brace_cols = [\"brace\" + str(i) for i in range(max_layers-1)] \n",
    "layer_cols = [\"layer_height\" + str(i) for i in range(max_layers-2)]\n",
    "transformed_columns += brace_cols\n",
    "transformed_columns += layer_cols\n",
    "\n",
    "dataframes = []\n",
    "for dataset, name in zip(DATA_ARRAY, DATA_NAMES):\n",
    "    encoding = [encode(d, max_layers, brace_dict, N_BRACES, one_hot=False, native=True, normalize_layer_heights=True) for d in dataset]\n",
    "    df_ = pd.DataFrame(encoding, columns=transformed_columns)\n",
    "    df_[\"label\"] = [name]*len(df_)\n",
    "    dataframes.append(df_.copy())\n",
    "    \n",
    "train_original = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "nominal_features = brace_cols\n",
    "ordinal_features = [\"n_layers\", \"legs\"]\n",
    "BERNOULLI = [\"legs\"]\n",
    "\n",
    "discrete_features = nominal_features + ordinal_features\n",
    "\n",
    "\n",
    "continuous_features = list(set(transformed_columns) - set(nominal_features) - set(ordinal_features))\n",
    "\n",
    "train_original[ordinal_features] = train_original[ordinal_features].astype(\"int\")\n",
    "train_original[continuous_features] = train_original[continuous_features].astype(\"float\")\n",
    "\n",
    "# synth_original[ordinal_features] = synth_original[ordinal_features].astype(\"int\")\n",
    "# synth_original[continuous_features] = synth_original[continuous_features].astype(\"float\")\n",
    "\n",
    "train = train_original.drop(\"label\", axis=1)\n",
    "\n",
    "\n",
    "def get_ordinal(n):\n",
    "    if 10 <= n % 100 <= 20:\n",
    "        suffix = 'th'\n",
    "    else:\n",
    "        suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "    return f\"{n}{suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtypes_dict = {'continuous': float, 'categorical': str, 'ordinal': int,\\\n",
    "              'bernoulli': int, 'binomial': int}\n",
    "    \n",
    "dtypes_dict_famd = {'continuous': float, 'categorical': str, 'ordinal': str,\\\n",
    "              'bernoulli': str, 'binomial': str}\n",
    "\n",
    "var_distrib, var_transform_only, le_dict, brace_cols, unique_braces = get_var_metadata(train, train_original, brace_cols, BERNOULLI)\n",
    "p = train.shape[1]\n",
    "dtype = {train.columns[j]: dtypes_dict_famd[var_transform_only[j]] for j in range(p)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GReaT(llm='distilgpt2', batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number of vertical legs', 'total height', 'bottom radius', 'top radius', 'number of layers', 'brace type of 1st layer', 'brace type of 2nd layer', 'brace type of 3rd layer', 'brace type of 4th layer', 'brace type of 5th layer', '1st layer height', '2nd layer height', '3rd layer height', '4th layer height']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f56c48e5c84cca8dacf2580a359a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 121.2799, 'train_samples_per_second': 82.454, 'train_steps_per_second': 3.298, 'train_loss': 0.8250865173339844, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "brace_cols_informative = [\"brace type of {} layer\".format(get_ordinal(i+1)) for i in range(len(brace_cols))]\n",
    "layer_cols_informative = [\"{} layer height\".format(get_ordinal(i+1)) for i in range(len(layer_cols))]\n",
    "columns_informative = [\"number of vertical legs\", \"total height\", \"bottom radius\", \"top radius\", \"number of layers\"]\n",
    "\n",
    "columns_informative += brace_cols_informative\n",
    "columns_informative += layer_cols_informative\n",
    "if USE_INFORMATIVE_NAMES:\n",
    "    train_informative = train.copy()\n",
    "    train_informative.columns = columns_informative\n",
    "    print(columns_informative)\n",
    "    model.fit(train_informative)\n",
    "else:\n",
    "    model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_nb_pseudo_obs = 20\n",
    "# sample 5 times because we loose some from constraints\n",
    "\n",
    "\n",
    "cols = train.columns\n",
    "\n",
    "\n",
    "layer_cols_ = [train.columns.get_loc(col) for col in layer_cols]\n",
    "brace_cols_ = [train.columns.get_loc(col) for col in brace_cols]\n",
    "\n",
    "\n",
    "\n",
    "other_continuous_cols = list(set(np.where(var_distrib == 'continuous')[0]) - set(layer_cols))\n",
    "min_values = train.min()\n",
    "max_values = train.max()\n",
    "\n",
    "ranges = np.array(list(zip(min_values, max_values)))\n",
    "ranges_cont = [ranges[other_continuous_cols], other_continuous_cols]\n",
    "\n",
    "\n",
    "def get_samples(target_nb_pseudo_obs):\n",
    "    y_new_all = []\n",
    "    total_nb_obs_generated = 0\n",
    "    nb_pseudo_obs = 0\n",
    "    nb_points = target_nb_pseudo_obs\n",
    "    while nb_pseudo_obs <= target_nb_pseudo_obs:\n",
    "        y_new = model.sample(n_samples=nb_points,max_length=4000)\n",
    "        if USE_INFORMATIVE_NAMES:\n",
    "            y_new.columns = train.columns\n",
    "        mask = np.apply_along_axis(lambda x: check_constraints(x, var_distrib, le_dict, cols, layer_cols_, brace_cols_, continuous_ranges=ranges_cont), axis=1, arr=y_new)\n",
    "        filtered_rows = y_new[mask]\n",
    "        y_new_all.append(filtered_rows)\n",
    "\n",
    "        total_nb_obs_generated += len(y_new)\n",
    "\n",
    "        nb_pseudo_obs = len(np.concatenate(y_new_all))\n",
    "\n",
    "        nb_points = target_nb_pseudo_obs - nb_pseudo_obs + target_nb_pseudo_obs//10\n",
    "\n",
    "    # Keep target_nb_pseudo_obs pseudo-observations\n",
    "    y_new_all = np.concatenate(y_new_all)\n",
    "    y_new_all = y_new_all[:target_nb_pseudo_obs]\n",
    "    return y_new_all\n",
    "\n",
    "# y_new_all = get_samples(target_nb_pseudo_obs)\n",
    "# pred = pd.DataFrame(y_new_all, columns = train.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mapply_along_axis(\u001b[39mlambda\u001b[39;00m x: check_constraints(x, var_distrib, le_dict, cols, layer_cols_, brace_cols_, continuous_ranges\u001b[39m=\u001b[39mranges_cont), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, arr\u001b[39m=\u001b[39my_new)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_new' is not defined"
     ]
    }
   ],
   "source": [
    "mask = np.apply_along_axis(lambda x: check_constraints(x, var_distrib, le_dict, cols, layer_cols_, brace_cols_, continuous_ranges=ranges_cont), axis=1, arr=y_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_post, pred_post_famd = post_process(pred, train.columns, le_dict, discrete_features, brace_cols, dtype, layer_cols_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_output = transform_df_to_json(pred_post)\n",
    "\n",
    "prfx = \"_informative\" if USE_INFORMATIVE_NAMES else \"\"\n",
    "out_name = \"llm(\"+\"_\".join(DATA_NAMES) + \")_\" + str(target_nb_pseudo_obs) + prfx +\".json\"\n",
    "output_filename = res_folder + out_name\n",
    "with open(output_filename, \"w\") as json_file:\n",
    "    json.dump(json_output, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_plots([\"real\", \"synthetic (GA)\", \"synthetic\"], train_original, pred_post, None, None, var_distrib, le_dict, brace_cols, unique_braces, res_folder)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n",
      "57it [00:46,  1.22it/s]                        \n",
      "100%|██████████| 28/28 [00:28<00:00,  1.01s/it]\n",
      "100%|██████████| 16/16 [00:13<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [06:47,  1.23it/s]                         \n",
      "324it [04:20,  1.24it/s]                         \n",
      "193it [02:49,  1.14it/s]                         \n",
      "96it [01:19,  1.21it/s]                        \n",
      "100%|██████████| 50/50 [00:41<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [13:50,  1.21it/s]                         \n",
      "100%|██████████| 609/609 [08:54<00:00,  1.14it/s]\n",
      "328it [04:24,  1.24it/s]                         \n",
      "180it [02:20,  1.28it/s]                         \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store results\n",
    "target_values = [100, 500, 1000]\n",
    "num_iterations = 1  # Number of times to run the function for each target value\n",
    "execution_times = []\n",
    "\n",
    "for target_nb_pseudo_obs in target_values:\n",
    "    times_for_target = []\n",
    "    print(f\"Processing target_nb_pseudo_obs = {target_nb_pseudo_obs}\")\n",
    "    start_time = time.time()\n",
    "    y_new_all = get_samples(target_nb_pseudo_obs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    execution_times.append(execution_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a file\n",
    "with open(res_folder+'execution_times.txt', 'w') as f:\n",
    "    for target_nb_pseudo_obs, mean_time in zip(target_values, execution_times):\n",
    "        f.write(f'Target obs: {target_nb_pseudo_obs}, Mean Time: {mean_time:.4f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
