{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003ff89d",
   "metadata": {},
   "source": [
    "To run on GPU activate conda environment (samplestructures) and run jupyter-notebook (not from VS code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0760496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luser/miniconda3/envs/samplestructures/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ON CPU\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon April 29 13:25:11 2020\n",
    "\n",
    "@author: rfuchs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import os \n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from gower import gower_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "from src.MIAMI import MIAMI\n",
    "from src.init_params import dim_reduce_init\n",
    "from src.data_preprocessing import compute_nj\n",
    "from src.utils_methods import encode, get_var_metadata, post_process, transform_df_to_json, generate_plots\n",
    "\n",
    "from shapely.geometry import Polygon as polygon\n",
    "\n",
    "import autograd.numpy as np\n",
    "\n",
    "import colorcet as cc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdae60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "main_folder = ''\n",
    "res_folder = main_folder + \"results/DGMM/\"\n",
    "\n",
    "data_folder = \"results/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "dtypes_dict = {'continuous': float, 'categorical': str, 'ordinal': int,\\\n",
    "              'bernoulli': int, 'binomial': int}\n",
    "    \n",
    "dtypes_dict_famd = {'continuous': float, 'categorical': str, 'ordinal': str,\\\n",
    "              'bernoulli': str, 'binomial': str}\n",
    "#===========================================#\n",
    "# Importing data\n",
    "#===========================================#\n",
    "\n",
    "inf_nb = 1E12\n",
    "\n",
    "sub_design = \"bivariate\"\n",
    "\n",
    "# acceptance_rate =\n",
    "\n",
    "def find_files_with_inf(directory_path):\n",
    "    result_list = []\n",
    "\n",
    "    # Iterate over each folder in the specified directory\n",
    "    for folder_name in os.listdir(directory_path):\n",
    "        folder_path = os.path.join(directory_path, folder_name)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # List all files in the folder\n",
    "            files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "            # Filter files with \"inf\" in their names\n",
    "            inf_files = [file for file in files_in_folder if \"inf\" in file]\n",
    "\n",
    "            # Add the folder path and corresponding \"inf\" files to the result list\n",
    "            result_list.append({\"folder_path\": folder_path, \"inf_files\": inf_files})\n",
    "\n",
    "    return result_list\n",
    "\n",
    "\n",
    "import json\n",
    "with open(data_folder + \"real/real_structures.json\") as f:\n",
    "    real_orig = json.load(f)\n",
    "# with open(data_folder + \"GA/optimal_run_pop_100_gen_2000_plausibility_FAMD GMM.json\") as f:\n",
    "#     ga_2000 = json.load(f)\n",
    "# with open(data_folder + \"GA/optimal_run_pop_100_gen_2000_plausibility_FAMD GMM_weighted_10.json\") as f:\n",
    "#     ga_plaus_famd_gmm_weighted_10 = json.load(f)\n",
    "# with open(data_folder + \"GA/optimal_run_pop_100_gen_2000_plausibility_FAMD GMM_weighted_100.json\") as f:\n",
    "#     ga_plaus_famd_gmm_weighted_100 = json.load(f)\n",
    "# with open(data_folder + \"GA/optimal_run_pop_100_gen_2000_plausibility_FAMD GMM_weighted_1000.json\") as f:\n",
    "#     ga_plaus_famd_gmm_weighted_1000 = json.load(f)\n",
    "# with open(data_folder + \"GA/optimal_run_pop_100_gen_2000_plausibility_FAMD GMM_weighted_10000.json\") as f:\n",
    "#     ga_plaus_famd_gmm_weighted_10000 = json.load(f)\n",
    "\n",
    "# DATA_ARRAY = [real_orig, ga_2000, ga_plaus_famd_gmm_weighted_10, ga_plaus_famd_gmm_weighted_100, ga_plaus_famd_gmm_weighted_1000, ga_plaus_famd_gmm_weighted_10000]\n",
    "# DATA_NAMES = [\"real\", \"GA-plausibility (FAMD GMM)\", \"GA-plausibility (FAMD GMM) - weighted plausibility 10\", \"GA-plausibility (FAMD GMM) - weighted plausibility 100\", \"GA-plausibility (FAMD GMM) - weighted plausibility 1000\", \"GA-plausibility (FAMD GMM) - weighted plausibility 10000\"]\n",
    "\n",
    "# with open(\"results_final/GA_done_final/ga_results_one class svm_nu_0.5/P[One Class SVM]_thr_inf_seed_0_evaluated.json\") as f:\n",
    "    # ga = json.load(f)\n",
    "\n",
    "result_ga_lists_inf = find_files_with_inf(\"results_final/GA_done_final/\")\n",
    "\n",
    "folders_to_load_from = [\"results_final/GA_done_final/kde\"]\n",
    "all_ga_inf = []\n",
    "for res_dict in result_ga_lists_inf:\n",
    "    folder_path = res_dict[\"folder_path\"]\n",
    "    if folder_path not in folders_to_load_from:\n",
    "        continue\n",
    "    for inf_file in res_dict[\"inf_files\"]:\n",
    "        with open(os.path.join(folder_path,inf_file)) as f:\n",
    "            ga = json.load(f)\n",
    "        all_ga_inf.extend(ga)\n",
    "print(len(all_ga_inf))\n",
    "\n",
    "\n",
    "with open(data_folder + \"evaluated/GA/P[None]_seed_0_evaluated.json\") as f:\n",
    "    ga_no_plaus = json.load(f)\n",
    "\n",
    "# DATA_ARRAY = [real_orig, all_ga_inf]\n",
    "# DATA_NAMES = [\"real\", \"all_ga_inf\"]\n",
    "# DATA_ARRAY = [real_orig, ga_no_plaus]\n",
    "# DATA_NAMES = [\"real\", \"ga_no_plaus\"]\n",
    "DATA_ARRAY = [real_orig]\n",
    "DATA_NAMES = [\"real\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b48a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDE\n"
     ]
    }
   ],
   "source": [
    "from src.plausibility_metrics import *\n",
    "discriminator = ClassKDE()\n",
    "real_scores = discriminator.get_real_plaus()\n",
    "median = np.median(real_scores)\n",
    "std = np.std(real_scores) \n",
    "threshold = median + 1*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e4c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "brace_dict = {    \n",
    "    \"NONE\": 0,\n",
    "    \"H\": 1,\n",
    "    \"Z\": 2,\n",
    "    \"IZ\": 3,\n",
    "    \"ZH\": 4,\n",
    "    \"IZH\": 5,\n",
    "    \"K\": 6,\n",
    "    \"X\": 7,\n",
    "    \"XH\": 8,\n",
    "    \"nan\": 9,\n",
    "}\n",
    "\n",
    "brace_dict_inv = dict(zip(brace_dict.values(), brace_dict.keys()))\n",
    "\n",
    "N_BRACES = len(brace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306fe5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = max(d.get('n_layers', 0) for sublist in DATA_ARRAY for d in sublist)\n",
    "transformed_columns = [\"legs\", \"total_height\", \"radius_bottom\", \"radius_top\", \"n_layers\"]\n",
    "brace_cols = [\"brace\" + str(i) for i in range(max_layers-1)] \n",
    "layer_cols = [\"layer_height\" + str(i) for i in range(max_layers-2)]\n",
    "transformed_columns += brace_cols\n",
    "transformed_columns += layer_cols\n",
    "\n",
    "dataframes = []\n",
    "for dataset, name in zip(DATA_ARRAY, DATA_NAMES):\n",
    "    encoding = [encode(d, max_layers, brace_dict, N_BRACES, one_hot=False, native=True, normalize_layer_heights=True) for d in dataset]\n",
    "    df_ = pd.DataFrame(encoding, columns=transformed_columns)\n",
    "    df_[\"label\"] = [name]*len(df_)\n",
    "    dataframes.append(df_.copy())\n",
    "    \n",
    "train_original = pd.concat(dataframes, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c40cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>legs</th>\n",
       "      <th>total_height</th>\n",
       "      <th>radius_bottom</th>\n",
       "      <th>radius_top</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>brace0</th>\n",
       "      <th>brace1</th>\n",
       "      <th>brace2</th>\n",
       "      <th>brace3</th>\n",
       "      <th>brace4</th>\n",
       "      <th>layer_height0</th>\n",
       "      <th>layer_height1</th>\n",
       "      <th>layer_height2</th>\n",
       "      <th>layer_height3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.968</td>\n",
       "      <td>15.554</td>\n",
       "      <td>2</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>77.0</td>\n",
       "      <td>21.362</td>\n",
       "      <td>11.761</td>\n",
       "      <td>3</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.5454545454545454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>32.9</td>\n",
       "      <td>20.704</td>\n",
       "      <td>6.64</td>\n",
       "      <td>3</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.5455927051671733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.166</td>\n",
       "      <td>8.706</td>\n",
       "      <td>3</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.48200000000000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>51.0</td>\n",
       "      <td>19.029</td>\n",
       "      <td>9.336</td>\n",
       "      <td>3</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.5490196078431373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>44.5</td>\n",
       "      <td>19.796</td>\n",
       "      <td>7.777</td>\n",
       "      <td>5</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.3258426966292135</td>\n",
       "      <td>0.5955056179775281</td>\n",
       "      <td>0.8202247191011236</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>43.5</td>\n",
       "      <td>18.382</td>\n",
       "      <td>7.777</td>\n",
       "      <td>5</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.3218390804597701</td>\n",
       "      <td>0.593103448275862</td>\n",
       "      <td>0.8160919540229885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>50.3</td>\n",
       "      <td>18.382</td>\n",
       "      <td>7.777</td>\n",
       "      <td>5</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.3101391650099404</td>\n",
       "      <td>0.5745526838966203</td>\n",
       "      <td>0.8051689860834991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>18.382</td>\n",
       "      <td>7.777</td>\n",
       "      <td>5</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.21346153846153845</td>\n",
       "      <td>0.4461538461538461</td>\n",
       "      <td>0.7346153846153847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>13.435</td>\n",
       "      <td>6.314</td>\n",
       "      <td>6</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4671428571428572</td>\n",
       "      <td>0.6671428571428571</td>\n",
       "      <td>0.8342857142857143</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   legs total_height radius_bottom radius_top n_layers brace0 brace1 brace2  \\\n",
       "0     4         22.0        16.968     15.554        2      X    nan    nan   \n",
       "1     3         77.0        21.362     11.761        3      X      X    nan   \n",
       "2     4         32.9        20.704       6.64        3      X      X    nan   \n",
       "3     3         50.0        16.166      8.706        3      X      X    nan   \n",
       "4     3         51.0        19.029      9.336        3      X      X    nan   \n",
       "..  ...          ...           ...        ...      ...    ...    ...    ...   \n",
       "95    4         44.5        19.796      7.777        5      X      X      X   \n",
       "96    4         43.5        18.382      7.777        5      X      X      X   \n",
       "97    4         50.3        18.382      7.777        5      X      X      X   \n",
       "98    4         52.0        18.382      7.777        5      X      X      X   \n",
       "99    4         70.0        13.435      6.314        6      X      X      X   \n",
       "\n",
       "   brace3 brace4        layer_height0       layer_height1       layer_height2  \\\n",
       "0     nan    nan                  1.0                 1.0                 1.0   \n",
       "1     nan    nan   0.5454545454545454                 1.0                 1.0   \n",
       "2     nan    nan   0.5455927051671733                 1.0                 1.0   \n",
       "3     nan    nan  0.48200000000000004                 1.0                 1.0   \n",
       "4     nan    nan   0.5490196078431373                 1.0                 1.0   \n",
       "..    ...    ...                  ...                 ...                 ...   \n",
       "95      X    nan   0.3258426966292135  0.5955056179775281  0.8202247191011236   \n",
       "96      X    nan   0.3218390804597701   0.593103448275862  0.8160919540229885   \n",
       "97      X    nan   0.3101391650099404  0.5745526838966203  0.8051689860834991   \n",
       "98      X    nan  0.21346153846153845  0.4461538461538461  0.7346153846153847   \n",
       "99      X      X                 0.25  0.4671428571428572  0.6671428571428571   \n",
       "\n",
       "         layer_height3 label  \n",
       "0                  1.0  real  \n",
       "1                  1.0  real  \n",
       "2                  1.0  real  \n",
       "3                  1.0  real  \n",
       "4                  1.0  real  \n",
       "..                 ...   ...  \n",
       "95                 1.0  real  \n",
       "96                 1.0  real  \n",
       "97                 1.0  real  \n",
       "98                 1.0  real  \n",
       "99  0.8342857142857143  real  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89bc526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 100 observations!!!!\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the two dataframes together and reindex\n",
    "# train_original = pd.concat([real, synthetic, rand], axis=0, ignore_index=True)\n",
    "\n",
    "# synth_original = pd.concat([synthetic], axis=0, ignore_index=True)\n",
    "\n",
    "# train_original = pd.concat([real], axis=0, ignore_index=True)\n",
    "\n",
    "nominal_features = brace_cols\n",
    "ordinal_features = [\"n_layers\", \"legs\"]\n",
    "BERNOULLI = [\"legs\"]\n",
    "\n",
    "discrete_features = nominal_features + ordinal_features\n",
    "\n",
    "\n",
    "continuous_features = list(set(transformed_columns) - set(nominal_features) - set(ordinal_features))\n",
    "\n",
    "train_original[ordinal_features] = train_original[ordinal_features].astype(\"int\")\n",
    "train_original[continuous_features] = train_original[continuous_features].astype(\"float\")\n",
    "\n",
    "# synth_original[ordinal_features] = synth_original[ordinal_features].astype(\"int\")\n",
    "# synth_original[continuous_features] = synth_original[continuous_features].astype(\"float\")\n",
    "\n",
    "train = train_original.drop(\"label\", axis=1)\n",
    "\n",
    "# synth = synth_original.drop(\"label\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = train.infer_objects()\n",
    "# synth = synth.infer_objects()\n",
    "\n",
    "numobs = len(train)\n",
    "print(\"Running with\", numobs, \"observations!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef631fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_distrib, var_transform_only, le_dict, brace_cols, unique_braces = get_var_metadata(train, train_original, brace_cols, BERNOULLI)\n",
    "\n",
    "\n",
    "\n",
    "nj, nj_bin, nj_ord, nj_categ = compute_nj(train, var_distrib)\n",
    "\n",
    "nb_cont = np.sum(var_distrib == 'continuous')     \n",
    "\n",
    "p = train.shape[1]\n",
    "        \n",
    "# Feature category (cf)\n",
    "dtype = {train.columns[j]: dtypes_dict_famd[var_transform_only[j]] for j in range(p)}\n",
    "\n",
    "train_famd = train.astype(dtype, copy=True)\n",
    "numobs = len(train)\n",
    "\n",
    "authorized_ranges = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb348df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bernoulli',\n",
       " 'continuous',\n",
       " 'continuous',\n",
       " 'continuous',\n",
       " 'ordinal',\n",
       " 'categorical',\n",
       " 'categorical',\n",
       " 'ordinal',\n",
       " 'ordinal',\n",
       " 'categorical',\n",
       " 'continuous',\n",
       " 'continuous',\n",
       " 'continuous',\n",
       " 'continuous']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_transform_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25fe5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bernoulli', 'continuous', 'continuous', 'continuous', 'ordinal',\n",
       "       'bernoulli', 'bernoulli', 'ordinal', 'ordinal', 'bernoulli',\n",
       "       'continuous', 'continuous', 'continuous', 'continuous'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75561601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legs               int64\n",
       "total_height     float64\n",
       "radius_bottom    float64\n",
       "radius_top       float64\n",
       "n_layers           int64\n",
       "brace0             int64\n",
       "brace1             int64\n",
       "brace2             int64\n",
       "brace3             int64\n",
       "brace4             int64\n",
       "layer_height0    float64\n",
       "layer_height1    float64\n",
       "layer_height2    float64\n",
       "layer_height3    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462a349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legs              object\n",
       "total_height     float64\n",
       "radius_bottom    float64\n",
       "radius_top       float64\n",
       "n_layers          object\n",
       "brace0            object\n",
       "brace1            object\n",
       "brace2            object\n",
       "brace3            object\n",
       "brace4            object\n",
       "layer_height0    float64\n",
       "layer_height1    float64\n",
       "layer_height2    float64\n",
       "layer_height3    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_famd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d76bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var_transform_only)\n",
    "print(var_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_design = \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a79837",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "# Model Hyper-parameters\n",
    "#===========================================#\n",
    "\n",
    "n_clusters = 3\n",
    "r = np.array([2, 1])\n",
    "k = [n_clusters]\n",
    "\n",
    "seed = 2023\n",
    "init_seed = 2024\n",
    "    \n",
    "# !!! Changed eps\n",
    "eps = 1E-10\n",
    "it = 1000\n",
    "maxstep = 1000\n",
    "target_nb_pseudo_obs = 100\n",
    "\n",
    "# print(train_famd)\n",
    "#*****************************************************************\n",
    "# Run MIAMI\n",
    "#*****************************************************************\n",
    "print(\"Initialize dimensionality reduction\")    \n",
    "init, transformed_famd_data, famd  = dim_reduce_init(train_famd, n_clusters, k, r, nj, var_distrib, seed = 2023,\\\n",
    "                                use_light_famd=True)\n",
    "\n",
    "print(init.keys())\n",
    "print(len(init[\"eta\"][0]))\n",
    "print(len(init[\"H\"]))\n",
    "\n",
    "print(\"Computing distance matrix\")\n",
    "# # Defining distances over the features\n",
    "# dm = gower_matrix(train, cat_features = cat_features) \n",
    "\n",
    "distances = pdist(transformed_famd_data)\n",
    "\n",
    "dm = squareform(distances)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdass = ['lambda_bin', 'lambda_ord', 'lambda_cont', 'lambda_categ']\n",
    "print(var_distrib)\n",
    "summ = 0\n",
    "for lambas in lambdass:\n",
    "    print(lambas, len(init[lambas]), np.array(init[lambas][0]).size)\n",
    "    summ += len(init[lambas]) * np.array(init[lambas][0]).size\n",
    "\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plausibility_filter = {\"clf\": discriminator, \"threshold\": threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MIAMI with\", train.shape)\n",
    "out = MIAMI(train, n_clusters, r, k, init, var_distrib, nj, le_dict, authorized_ranges, discrete_features, dtype, target_nb_pseudo_obs=target_nb_pseudo_obs, it=it,\\\n",
    "                eps=eps, maxstep=maxstep, seed=seed, perform_selec = True, dm = dm, max_patience = 0)\n",
    "print('MIAMI has kept one observation over', round(1 / out['share_kept_pseudo_obs']),\\\n",
    "        'observations generated')\n",
    "    \n",
    "acceptance_rate = out['share_kept_pseudo_obs']\n",
    "print(acceptance_rate)\n",
    "pred = pd.DataFrame(out['y_all'], columns = train.columns) \n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd02d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_z_s_from_y(y, mu_s, sigma_s, eta, M):\n",
    "    ''' Generate z_s and zc_s using input data y instead of drawing random values.\n",
    "    y (ndarray): Input data\n",
    "    mu_s (list of nd-arrays): The means of the Gaussians starting at each layer\n",
    "    sigma_s (list of nd-arrays): The covariance matrices of the Gaussians \n",
    "                                 starting at each layer\n",
    "    eta (list of nb_layers elements of shape (K_l x r_{l-1}, 1)): mu parameters\n",
    "                                                                 for each layer\n",
    "    M (list of int): The number of MC to generate on each layer\n",
    "    -------------------------------------------------------------------------\n",
    "    returns (list of ndarrays): z^{l} | s for all s in Omega and all l in L\n",
    "    '''\n",
    "\n",
    "    L = len(mu_s) - 1    \n",
    "    r = [mu_s[l].shape[1] for l in range(L + 1)]\n",
    "    S = [mu_s[l].shape[0] for l in range(L + 1)]\n",
    "    \n",
    "    z_s = []\n",
    "    zc_s = []  # z centered (denoted c) or all l\n",
    "\n",
    "    for l in range(L + 1):\n",
    "        if l == 0:\n",
    "            zl_s = y[:, np.newaxis, :, np.newaxis]  # Reshape y for the first layer\n",
    "        else:\n",
    "            zl_s = multivariate_normal(size=(M[l], 1),\n",
    "                                       mean=mu_s[l].flatten(order='C'),\n",
    "                                       cov=block_diag(*sigma_s[l]))\n",
    "        zl_s = zl_s.reshape(M[l], S[l], r[l], order='C')\n",
    "        z_s.append(t(zl_s, (0, 2, 1)))\n",
    "\n",
    "        if l < L:  # The last layer is already centered\n",
    "            eta_ = np.repeat(t(eta[l], (2, 0, 1)), S[l + 1], axis=1)\n",
    "            zc_s.append(zl_s - eta_)\n",
    "\n",
    "    return z_s, zc_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89487e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns\n",
    "layer_cols_ = np.array([i for i, s in enumerate(cols) if s.startswith('layer_height')])\n",
    "brace_cols_ = np.array([i for i, s in enumerate(cols) if s.startswith('brace')])\n",
    "\n",
    "pred_post, pred_post_famd = post_process(pred, train.columns, le_dict, discrete_features, brace_cols, dtype, layer_cols_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_output = transform_df_to_json(pred_post)\n",
    "out_name = \"dgmm(\"+\"_\".join(DATA_NAMES) + \")_\" + str(target_nb_pseudo_obs) + \".json\"\n",
    "output_filename = res_folder + out_name\n",
    "with open(output_filename, \"w\") as json_file:\n",
    "    json.dump(json_output, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8244bbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# zz = out[\"zz\"]\n",
    "# npzz = np.array(zz)\n",
    "# npzz = np.reshape(npzz, (npzz.shape[0], npzz.shape[-1]))\n",
    "\n",
    "# mean = np.mean(npzz, axis=0)\n",
    "# var = np.std(npzz, axis=0)\n",
    "# var_s = 2\n",
    "\n",
    "\n",
    "# x_lim = [mean[0] - var_s*var[0], mean[0] + var_s*var[0]]\n",
    "# y_lim = [mean[1] - var_s*var[1], mean[1] + var_s*var[1]]\n",
    "\n",
    "\n",
    "def plot_discr(out, train_original, x_lim=[], y_lim=[]):\n",
    "    df_data = pd.DataFrame()\n",
    "    df_generated = pd.DataFrame()\n",
    "    \n",
    "    df_data[\"x\"] = out['Ez.y'][:,0]\n",
    "    df_data[\"y\"] = out['Ez.y'][:,1]\n",
    "    \n",
    "    zz = out['zz']\n",
    "#     zz = famd.transform(pred_famd)\n",
    "    \n",
    "    df_generated[\"x\"],  df_generated[\"y\"] = zz[:,0], zz[:,1]\n",
    "    \n",
    "    df_generated[\"method\"] = \"synthetic\"\n",
    "    \n",
    "    df_data[\"method\"] = train_original[\"label\"]\n",
    "    \n",
    "    df_scatter = pd.concat([df_data, df_generated], axis=0, ignore_index=True)\n",
    "\n",
    "    not_real_inp = df_data[df_data['method'] != 'real']\n",
    "    real_inp = df_data[df_data['method'] == 'real']\n",
    "\n",
    "    not_real_all = pd.concat([not_real_inp, df_generated], axis=0, ignore_index=True)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    # fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # ax = sns.scatterplot(data=df_scatter, x=\"x\", y=\"y\", style=\"method\", hue=\"method\", alpha=0.7)\n",
    "    # Create a scatter plot\n",
    "\n",
    "    # Create a KDE plot on the same Axes\n",
    "    g = sns.displot(data=not_real_all, x=\"x\", y=\"y\", hue=\"method\", alpha=0.5, kind=\"kde\")\n",
    "    g1 = sns.scatterplot(data=real_inp, x=\"x\", y=\"y\", label=\"real\", color=\"black\", s=60, ax=g.ax)\n",
    "\n",
    "    # # Get the legends from both plots\n",
    "    # legend1 = g.ax.get_legend()\n",
    "    # legend2 = g1.get_legend()\n",
    "\n",
    "    # # Concatenate the legends\n",
    "    # all_legends = legend1.legendHandles + legend2.legendHandles\n",
    "\n",
    "    # # Set the concatenated legend\n",
    "    # g.ax.legend(all_legends, ['Legend1', 'Legend2'])\n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # kde_legend = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='KDE Legend')\n",
    "    # legend = ax.legend(handles=[kde_legend], labels=['KDE Legend'], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # Add a title and axis labels\n",
    "    # ax.set_title('Scatter plot grouped by type')\n",
    "    # ax.set_xlabel('PC1')\n",
    "    # ax.set_ylabel('PC2')\n",
    "\n",
    "    # # Create a legend\n",
    "    # # legend = ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), title=\"Data\", borderaxespad=0.)\n",
    "    # if len(x_lim)>0:\n",
    "    #     ax.set_xlim(x_lim[0], x_lim[1])\n",
    "    #     ax.set_ylim(y_lim[0], y_lim[1])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_discr(out, train_original)\n",
    "# plot_discr(out, train_original, x_lim=x_lim, y_lim=y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3de0a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_plot = DATA_NAMES + [\"synthetic\"]\n",
    "generate_plots(to_plot, train_original, pred_post, out['Ez.y'], out['zz'], var_distrib, le_dict, brace_cols, unique_braces, res_folder, percentage=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_post_famd = pred_post.astype(dtype, copy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_zs_famd = famd.transform(pred_post_famd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7942396",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "new_famd = pd.DataFrame()\n",
    "real_famd = pd.DataFrame()\n",
    "\n",
    "\n",
    "real_famd[\"x\"] = transformed_famd_data[:,0]\n",
    "real_famd[\"y\"] = transformed_famd_data[:,1]\n",
    "\n",
    "\n",
    "new_famd[\"x\"] = new_zs_famd[:,0]\n",
    "new_famd[\"y\"] = new_zs_famd[:,1]\n",
    "\n",
    "ax = sns.scatterplot(data=new_famd, x=\"x\", y=\"y\")\n",
    "ax = sns.scatterplot(data=real_famd, x=\"x\", y=\"y\")\n",
    "\n",
    "ax = sns.scatterplot(data=new_famd.iloc[0:1], x=\"x\", y=\"y\")\n",
    "\n",
    "ax = sns.scatterplot(data=new_famd.iloc[-2:-1], x=\"x\", y=\"y\")\n",
    "\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax.set_title('Scatter plot grouped by type')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame()\n",
    "df_data[\"x\"] = out['Ez.y'][:,0]\n",
    "df_data[\"y\"] = out['Ez.y'][:,1]\n",
    "\n",
    "zz = out['zz']\n",
    "df_generated = pd.DataFrame()\n",
    "df_generated[\"x\"],  df_generated[\"y\"] = zz[:,0], zz[:,1]\n",
    "\n",
    "ax = sns.scatterplot(data=df_data, x=\"x\", y=\"y\")\n",
    "ax = sns.scatterplot(data=df_generated, x=\"x\", y=\"y\")\n",
    "\n",
    "ax = sns.scatterplot(data=df_generated.iloc[0:1], x=\"x\", y=\"y\")\n",
    "ax = sns.scatterplot(data=df_generated.iloc[-2:-1], x=\"x\", y=\"y\")\n",
    "\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax.set_title('Scatter plot grouped by type')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.linalg import inv\n",
    "\n",
    "# Calculate mean and covariance matrix of the data\n",
    "mean = np.mean(transformed_famd_data, axis=0)\n",
    "covariance_matrix = np.cov(transformed_famd_data, rowvar=False)\n",
    "\n",
    "# Calculate the inverse of the covariance matrix\n",
    "inv_cov_matrix = inv(covariance_matrix)\n",
    "\n",
    "# New test instance\n",
    "test_instance1 = np.array(new_zs_famd[0])\n",
    "\n",
    "test_instance2 = np.array(new_zs_famd[-1])\n",
    "\n",
    "# Calculate Mahalanobis distance\n",
    "mahalanobis_distance1 = mahalanobis(test_instance1, mean, inv_cov_matrix)\n",
    "mahalanobis_distance2 = mahalanobis(test_instance2, mean, inv_cov_matrix)\n",
    "print(mahalanobis_distance1, mahalanobis_distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "mu = out['mu'][0]\n",
    "sigma = out['sigma'][0]\n",
    "weights = out['w_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ce01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu.shape)\n",
    "print(sigma.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle=angle, **kwargs))\n",
    "\n",
    "def plot_discr(out, train_original, x_lim=[], y_lim=[], elipse=True):\n",
    "    df_data = pd.DataFrame()\n",
    "    df_generated = pd.DataFrame()\n",
    "    \n",
    "    df_data[\"x\"] = out['Ez.y'][:,0]\n",
    "    df_data[\"y\"] = out['Ez.y'][:,1]\n",
    "    \n",
    "    zz = out['zz']\n",
    "    \n",
    "#     zz = famd.transform(pred_famd)\n",
    "    \n",
    "    df_generated[\"x\"],  df_generated[\"y\"] = zz[:,0], zz[:,1]\n",
    "    \n",
    "    df_generated[\"method\"] = \"synthetic (MIAMI)\"\n",
    "    \n",
    "    df_data[\"method\"] = train_original[\"label\"]\n",
    "    \n",
    "    df_scatter = pd.concat([df_data, df_generated], axis=0, ignore_index=True)\n",
    "    \n",
    "\n",
    "    # Create a scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax = sns.scatterplot(data=df_scatter, x=\"x\", y=\"y\", style=\"method\")\n",
    "    # Add a title and axis labels\n",
    "    ax.set_title('Scatter plot grouped by type')\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "\n",
    "    # Create a legend\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), title=\"Data\", borderaxespad=0.)\n",
    "    if len(x_lim)>0:\n",
    "        ax.set_xlim(x_lim[0], x_lim[1])\n",
    "        ax.set_ylim(y_lim[0], y_lim[1])\n",
    "        \n",
    "    if elipse:\n",
    "        weights_ = out['w_s']  \n",
    "        means_ = out['mu'][0]\n",
    "        covariances_ = out['sigma'][0]\n",
    "        \n",
    "        w_factor = 0.2 / weights_.max()\n",
    "        for pos, covar, w in zip(means_, covariances_, weights_):\n",
    "            draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "    plt.show()\n",
    "\n",
    "plot_discr(out, train_original)\n",
    "# plot_discr(out, train_original, x_lim=x_lim, y_lim=y_lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constraints(y, var_distrib, le_dict, colnames, layer_height_cols, brace_cols, continuous_ranges=None):\n",
    "    cont = y[var_distrib == 'continuous']\n",
    "    \n",
    "    \n",
    "#     y_trans = le_dict[colname].inverse_transform(pred[colname].astype(int))\n",
    "    \n",
    "    if (cont<0).any():\n",
    "        return False\n",
    "    if continuous_ranges is not None:\n",
    "        ranges = continuous_ranges[0]\n",
    "        column_ranges = continuous_ranges[1]\n",
    "        values = y[column_ranges]\n",
    "        \n",
    "        within_ranges = np.logical_and(values >= ranges[:, 0], values <= ranges[:, 1])\n",
    "\n",
    "        if not within_ranges.all():\n",
    "            return False\n",
    "    if y[colnames == 'radius_bottom'] < y[colnames == 'radius_top']:\n",
    "        # radius bottom bigger than radius top\n",
    "        return False\n",
    "    \n",
    "    n_layers = int(y[colnames == 'n_layers'])\n",
    "    n_layers_decode = int(le_dict['n_layers'].inverse_transform([n_layers])[0])\n",
    "    n_layer_corrected = n_layers_decode - 2\n",
    "    braces_corrected = n_layers_decode - 1\n",
    "    relevant_brace = brace_cols[:braces_corrected]\n",
    "    relevant_brace_cols = zip(y[relevant_brace], colnames[relevant_brace])\n",
    "    transf_brace = [le_dict[brace_col_name].inverse_transform([int(brace_col)])[0] for brace_col, brace_col_name in relevant_brace_cols]\n",
    "    if \"nan\" in transf_brace:\n",
    "        return False\n",
    "    \n",
    "    if n_layer_corrected>0:\n",
    "        layer_heights = y[layer_height_cols]\n",
    "        relevant_layer_heights = layer_heights[:n_layer_corrected]\n",
    "        \n",
    "        if (relevant_layer_heights>=1).any():\n",
    "            return False\n",
    "        is_ascending = all(relevant_layer_heights[i] <= relevant_layer_heights[i + 1] for i in range(len(relevant_layer_heights) - 1))\n",
    "        if not is_ascending:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.oversample import draw_new_bin, draw_new_ord,\\\n",
    "                       draw_new_categ, draw_new_cont\n",
    "\n",
    "def generate_pseudo(point, out, nj, num_points=10, r=0.5):  \n",
    "    \n",
    "    x_val = out['Ez.y'][point,0]\n",
    "    y_val = out['Ez.y'][point,1]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    samples_total = 5*num_points # sample 5 times more than wanted because we loose some\n",
    "    z = sample_points_in_circle(x_val, y_val, r, samples_total)\n",
    "    \n",
    "    lambda_bin = np.array(out['lambda_bin']) \n",
    "    lambda_ord = out['lambda_ord'] \n",
    "    lambda_categ = out['lambda_categ'] \n",
    "    lambda_cont = np.array(out['lambda_cont'])\n",
    "    \n",
    "    nj_bin = nj[pd.Series(var_distrib).isin(['bernoulli', 'binomial'])].astype(int)\n",
    "    nj_ord = nj[var_distrib == 'ordinal'].astype(int)\n",
    "    nj_categ = nj[var_distrib == 'categorical'].astype(int)\n",
    "\n",
    "    y_std = out['y_std']\n",
    "    \n",
    "    y_bin_new = []\n",
    "    y_categ_new = []\n",
    "    y_ord_new = []\n",
    "    y_cont_new = []\n",
    "\n",
    "\n",
    "    y_bin_new.append(draw_new_bin(lambda_bin, z, nj_bin))\n",
    "    y_categ_new.append(draw_new_categ(lambda_categ, z, nj_categ))\n",
    "    y_ord_new.append(draw_new_ord(lambda_ord, z, nj_ord))\n",
    "    y_cont_new.append(draw_new_cont(lambda_cont, z))\n",
    "    # Stack the quantities\n",
    "    y_bin_new = np.vstack(y_bin_new)\n",
    "    y_categ_new = np.vstack(y_categ_new)\n",
    "    y_ord_new = np.vstack(y_ord_new)\n",
    "    y_cont_new = np.vstack(y_cont_new)\n",
    "\n",
    "    # \"Destandardize\" the continous data\n",
    "    y_cont_new = y_cont_new * y_std\n",
    "\n",
    "    # Put them in the right order and append them to y\n",
    "    type_counter = {'count': 0, 'ordinal': 0,\\\n",
    "                    'categorical': 0, 'continuous': 0} \n",
    "\n",
    "    y_new = np.full((samples_total, out['y_shape'][1]), np.nan)\n",
    "    # Quite dirty:\n",
    "    for j, var in enumerate(var_distrib):\n",
    "        if (var == 'bernoulli') or (var == 'binomial'):\n",
    "            y_new[:, j] = y_bin_new[:, type_counter['count']]\n",
    "            type_counter['count'] =  type_counter['count'] + 1\n",
    "        elif var == 'ordinal':\n",
    "            y_new[:, j] = y_ord_new[:, type_counter[var]]\n",
    "            type_counter[var] =  type_counter[var] + 1\n",
    "        elif var == 'categorical':\n",
    "            y_new[:, j] = y_categ_new[:, type_counter[var]]\n",
    "            type_counter[var] =  type_counter[var] + 1\n",
    "        elif var == 'continuous':\n",
    "            y_new[:, j] = y_cont_new[:, type_counter[var]]\n",
    "            type_counter[var] =  type_counter[var] + 1\n",
    "        else:\n",
    "            raise ValueError(var, 'Type not implemented')\n",
    "    cols = out['cols']\n",
    "    layer_cols = np.array([i for i, s in enumerate(cols) if s.startswith('layer_height')])\n",
    "    brace_cols = np.array([i for i, s in enumerate(cols) if s.startswith('brace')])\n",
    "    other_continuous_cols = list(set(np.where(var_distrib == 'continuous')[0]) - set(layer_cols))\n",
    "    ranges_cont = [out['ranges'][other_continuous_cols], other_continuous_cols]\n",
    "    \n",
    "    mask = np.apply_along_axis(lambda x: check_constraints(x, var_distrib, le_dict, cols, layer_cols, brace_cols, continuous_ranges=ranges_cont), axis=1, arr=y_new)\n",
    "    filtered_rows = y_new[mask]\n",
    "    filtered_z = z[mask]\n",
    "    filtered_rows = filtered_rows[:num_points,:]\n",
    "    filtered_z = filtered_z[:num_points,:]\n",
    "    return filtered_rows, filtered_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144713a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_in_circle(center_x, center_y, radius, num_points):\n",
    "    # Generate random polar coordinates\n",
    "    theta = 2 * np.pi * np.random.rand(num_points)\n",
    "    r = radius * np.sqrt(np.random.rand(num_points))\n",
    "    \n",
    "    # Convert polar coordinates to Cartesian coordinates\n",
    "    x = center_x + r * np.cos(theta)\n",
    "    y = center_y + r * np.sin(theta)\n",
    "    \n",
    "    return np.array(list(zip(x, y)))\n",
    "\n",
    "def plot_near(z_points, item, x_lim=[], y_lim=[], elipse=True, save=False):\n",
    "    df_data = pd.DataFrame()\n",
    "    df_generated = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    df_data[\"x\"] = out['Ez.y'][:,0]\n",
    "    df_data[\"y\"] = out['Ez.y'][:,1]\n",
    "    \n",
    "    \n",
    "#     zz = famd.transform(pred_famd)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_data[\"method\"] = \"real\"\n",
    "    df_data[\"size\"] = 1\n",
    "    \n",
    "    df_scatter = pd.concat([df_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    df_item = pd.concat([df_scatter.loc[item:item]], axis=0, ignore_index=True) \n",
    "    \n",
    "    df_item[\"method\"] = \"selected\"\n",
    "    df_item[\"size\"] = 2\n",
    "\n",
    "\n",
    "    points = z_points\n",
    "    df_points = pd.DataFrame(points, columns=[\"x\", \"y\"])\n",
    "    df_points[\"method\"] = \"sampled\"\n",
    "    df_points[\"size\"] = 3\n",
    "    \n",
    "    # all_dfs = [df_scatter, df_item, df_points]\n",
    "    all_dfs = [df_scatter, df_item, df_points]\n",
    "\n",
    "    df_all = pd.concat(all_dfs, axis=0, ignore_index=True )\n",
    "\n",
    "    # Create a scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    # ax = sns.scatterplot(data=df_all, x=\"x\", y=\"y\", hue=\"method\", style=\"method\", size=\"method\", sizes=(50,200))\n",
    "    plt.scatter(df_scatter[\"x\"], df_scatter[\"y\"], marker=\"o\", s=30, label=\"Real data\", alpha=0.5)\n",
    "    plt.scatter(df_item[\"x\"], df_item[\"y\"], marker=\"X\", c=\"red\", s=200, label=\"Selected instance\", alpha=1)\n",
    "    plt.scatter(df_points[\"x\"], df_points[\"y\"], marker=\"x\", s=70, label=\"Sampled synthetic\", alpha=1)\n",
    "    # Add a title and axis labels\n",
    "    # ax.set_title('Scatter plot grouped by type')\n",
    "    ax.set_xlabel('Dim 0', fontsize=20)\n",
    "    ax.set_ylabel('Dim 1', fontsize=20)\n",
    "\n",
    "    methods = df_all[\"method\"].unique()\n",
    "    # Customize the legend labels\n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # indices = np.where(np.isin(labels, methods))[0].tolist()\n",
    "    # handles = list(np.array(handles)[indices])\n",
    "    # labels = list(np.array(labels)[indices])\n",
    "    # handles = handles[1:4]\n",
    "    # labels = labels[1:4]\n",
    "    # plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), title=\"Data\", borderaxespad=0.)\n",
    "    # plt.legend(fontsize=20, loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    plt.legend(fontsize=20, loc='lower left')\n",
    "    if len(x_lim)>0:\n",
    "        ax.set_xlim(x_lim[0], x_lim[1])\n",
    "        ax.set_ylim(y_lim[0], y_lim[1])\n",
    "        \n",
    "    if elipse:\n",
    "        weights_ = out['w_s']  \n",
    "        means_ = out['mu'][0]\n",
    "        covariances_ = out['sigma'][0]\n",
    "        \n",
    "        w_factor = 0.2 / weights_.max()\n",
    "        for pos, covar, w in zip(means_, covariances_, weights_):\n",
    "            draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title(\"Latent space\", fontsize=20)\n",
    "    if save:\n",
    "        plt.savefig(\"sampling.png\", dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "    return points\n",
    "\n",
    "num_points=20\n",
    "POINT = 68\n",
    "R = 0.6\n",
    "# generated_close, z_points = generate_pseudo(POINT, out, nj, num_points=num_points, r=R)\n",
    "z_points = plot_near(z_points, POINT, x_lim=[-2.5, 2.5], y_lim=[-2.5, 2.5], save=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891aea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# Inverse transform the datasets\n",
    "#================================================================\n",
    "generated_close_df = pd.DataFrame(generated_close, columns = train.columns) \n",
    "generated_close_trans = generated_close_df.copy()\n",
    "\n",
    "for j, colname in enumerate(train.columns):\n",
    "    if colname in le_dict.keys():\n",
    "        generated_close_trans[colname] = le_dict[colname].inverse_transform(generated_close_df[colname].astype(int))\n",
    "    \n",
    "generated_close_post = generated_close_trans.copy()    \n",
    "\n",
    "\n",
    "layer_height_cols = generated_close_post.filter(like='layer_height').columns.tolist()\n",
    "\n",
    "# Define a function to apply to each row\n",
    "def replace_layer_height(row, brace_cols, layer_height_cols):\n",
    "    n_layers = int(row['n_layers'])\n",
    "    layers_set_one = layer_height_cols[n_layers-2:]\n",
    "    braces_set_nan = brace_cols[n_layers-1:]\n",
    "    # Set the layer_height columns to 1.0 for all rows after the nth row\n",
    "    row[layers_set_one] = 1.0\n",
    "    row[braces_set_nan] = \"nan\"\n",
    "    \n",
    "    # Return the modified row\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "generated_close_post = generated_close_post.apply(lambda x: replace_layer_height(x, brace_cols, layer_height_cols), axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "generated_close_post[\"label\"] = \"sampled\"\n",
    "\n",
    "\n",
    "points_post = pd.concat([generated_close_post, train_original[POINT:POINT+1]],ignore_index=True)\n",
    "json_output = transform_df_to_json(points_post)\n",
    "# with open(res_folder + \"close_point_generated.json\", \"w\") as json_file:\n",
    "#     json.dump(json_output, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b00a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_close_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad64cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original[POINT:POINT+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eeec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.MIAMI import get_samples\n",
    "\n",
    "# Initialize lists to store results\n",
    "target_values = range(100, 10001, 100)\n",
    "num_iterations = 1  # Number of times to run the function for each target value\n",
    "execution_times = []\n",
    "lambda_bin, lambda_ord, lambda_categ, lambda_cont = out[\"lambda_bin\"], out[\"lambda_ord\"], out[\"lambda_categ\"], out[\"lambda_cont\"]\n",
    "y_std = out[\"y_std\"]\n",
    "other_continuous_cols = list(set(np.where(var_distrib == 'continuous')[0]) - set(layer_cols))\n",
    "ranges_cont = [out['ranges'][other_continuous_cols], other_continuous_cols]\n",
    "w = out['best_w_s']\n",
    "layer_cols__ = np.array([i for i, s in enumerate(cols) if s.startswith('layer_height')])\n",
    "brace_cols__ = np.array([i for i, s in enumerate(cols) if s.startswith('brace')])\n",
    "# for target_nb_pseudo_obs in target_values:\n",
    "#     times_for_target = []\n",
    "#     for iteration in range(num_iterations):\n",
    "#         print(f\"Processing target_nb_pseudo_obs = {target_nb_pseudo_obs}, Iteration {iteration+1}/{num_iterations}...\")\n",
    "#         start_time = time.time()\n",
    "#         y_new_all = get_samples(train, target_nb_pseudo_obs, out, var_distrib, p, k, r, mu, sigma, w, lambda_bin, lambda_ord,\\\n",
    "#                 lambda_categ, lambda_cont, nj_bin, nj_ord, nj_categ, y_std, le_dict, cols, layer_cols__, brace_cols__, ranges_cont)\n",
    "#         end_time = time.time()\n",
    "#         execution_time = end_time - start_time\n",
    "#         times_for_target.append(execution_time)\n",
    "\n",
    "#     execution_times.append(times_for_target)\n",
    "\n",
    "# # Calculate mean and standard deviation for each target value\n",
    "# mean_execution_times = [np.mean(times) for times in execution_times]\n",
    "# std_execution_times = [np.std(times) for times in execution_times]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def generate_seeds(target_nb_pseudo_obs, num_seeds=5):\n",
    "#     for seed in range(num_seeds):\n",
    "#         out_s = get_samples(train, target_nb_pseudo_obs, out, var_distrib, p, k, r, mu, sigma, w, lambda_bin, lambda_ord,\\\n",
    "#                 lambda_categ, lambda_cont, nj_bin, nj_ord, nj_categ, y_std, le_dict, cols, layer_cols__, brace_cols__, ranges_cont)        \n",
    "#         pred = pd.DataFrame(out_s['y_all'], columns = train.columns) \n",
    "#         pred_post, pred_post_famd = post_process(pred, train.columns, le_dict, discrete_features, brace_cols, dtype, layer_cols_)\n",
    "#         json_output = transform_df_to_json(pred_post)\n",
    "#         outfolder = res_folder + \"seeds/\" + str(target_nb_pseudo_obs) +\"/\"\n",
    "#         if not os.path.exists(outfolder):\n",
    "#             os.makedirs(outfolder)\n",
    "#         out_name = \"dgmm(\"+\"_\".join(DATA_NAMES) + \")_\" + str(target_nb_pseudo_obs) + \"_seed_\" + str(seed) + \".json\"\n",
    "#         output_filename = outfolder + out_name\n",
    "#         with open(output_filename, \"w\") as json_file:\n",
    "#             json.dump(json_output, json_file, indent=4)\n",
    "\n",
    "# checkpoints = [100, 500, 1000, 1500, 2000]\n",
    "\n",
    "# for checkpoint in checkpoints:\n",
    "#     generate_seeds(checkpoint, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save results to a file\n",
    "# with open(res_folder+'execution_times.txt', 'w') as f:\n",
    "#     for target_nb_pseudo_obs, mean_time, std_time in zip(target_values, mean_execution_times, std_execution_times):\n",
    "#         f.write(f'Target obs: {target_nb_pseudo_obs}, Mean Time: {mean_time:.4f}, Std Dev: {std_time:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f537de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the results with mean and std\n",
    "# plt.plot(target_values, mean_execution_times, label='Mean Execution Time', color='b')\n",
    "# plt.fill_between(target_values, \n",
    "#                  np.array(mean_execution_times) - np.array(std_execution_times), \n",
    "#                  np.array(mean_execution_times) + np.array(std_execution_times), \n",
    "#                  color='b', alpha=0.2, label='Standard Deviation')\n",
    "# plt.xlabel('Target Pseudo Observations')\n",
    "# plt.ylabel('Execution Time (seconds)')\n",
    "# plt.title('Execution Time with Uncertainty vs. Target Pseudo Observations')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
