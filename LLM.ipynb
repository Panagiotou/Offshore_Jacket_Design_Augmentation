{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ON GPU\n"
     ]
    }
   ],
   "source": [
    "from be_great import GReaT\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils_methods import encode, get_var_metadata, post_process, transform_df_to_json, generate_plots, check_constraints\n",
    "import be_great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/panagiotou/anaconda3/envs/samplestructures/lib/python3.11/site-packages/be_great/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(be_great.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = ''\n",
    "USE_INFORMATIVE_NAMES = False\n",
    "if USE_INFORMATIVE_NAMES:\n",
    "    res_folder = main_folder + \"results/LLM_informative/\"\n",
    "else:\n",
    "    res_folder = main_folder + \"results/LLM/\"\n",
    "data_folder = main_folder + \"data/\"  \n",
    "\n",
    "with open(\"real_structures.json\") as f:\n",
    "    real_orig = json.load(f)\n",
    "DATA_ARRAY = [real_orig]\n",
    "DATA_NAMES = [\"real\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brace_dict = {    \n",
    "    \"NONE\": 0,\n",
    "    \"H\": 1,\n",
    "    \"Z\": 2,\n",
    "    \"IZ\": 3,\n",
    "    \"ZH\": 4,\n",
    "    \"IZH\": 5,\n",
    "    \"K\": 6,\n",
    "    \"X\": 7,\n",
    "    \"XH\": 8,\n",
    "    \"nan\": 9,\n",
    "}\n",
    "\n",
    "brace_dict_inv = dict(zip(brace_dict.values(), brace_dict.keys()))\n",
    "\n",
    "N_BRACES = len(brace_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = max(d.get('n_layers', 0) for sublist in DATA_ARRAY for d in sublist)\n",
    "transformed_columns = [\"legs\", \"total_height\", \"radius_bottom\", \"radius_top\", \"n_layers\"]\n",
    "brace_cols = [\"brace\" + str(i) for i in range(max_layers-1)] \n",
    "layer_cols = [\"layer_height\" + str(i) for i in range(max_layers-2)]\n",
    "transformed_columns += brace_cols\n",
    "transformed_columns += layer_cols\n",
    "\n",
    "dataframes = []\n",
    "for dataset, name in zip(DATA_ARRAY, DATA_NAMES):\n",
    "    encoding = [encode(d, max_layers, brace_dict, N_BRACES, one_hot=False, native=True, normalize_layer_heights=True) for d in dataset]\n",
    "    df_ = pd.DataFrame(encoding, columns=transformed_columns)\n",
    "    df_[\"label\"] = [name]*len(df_)\n",
    "    dataframes.append(df_.copy())\n",
    "    \n",
    "train_original = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "nominal_features = brace_cols\n",
    "ordinal_features = [\"n_layers\", \"legs\"]\n",
    "BERNOULLI = [\"legs\"]\n",
    "\n",
    "discrete_features = nominal_features + ordinal_features\n",
    "\n",
    "\n",
    "continuous_features = list(set(transformed_columns) - set(nominal_features) - set(ordinal_features))\n",
    "\n",
    "train_original[ordinal_features] = train_original[ordinal_features].astype(\"int\")\n",
    "train_original[continuous_features] = train_original[continuous_features].astype(\"float\")\n",
    "\n",
    "# synth_original[ordinal_features] = synth_original[ordinal_features].astype(\"int\")\n",
    "# synth_original[continuous_features] = synth_original[continuous_features].astype(\"float\")\n",
    "\n",
    "train = train_original.drop(\"label\", axis=1)\n",
    "\n",
    "\n",
    "def get_ordinal(n):\n",
    "    if 10 <= n % 100 <= 20:\n",
    "        suffix = 'th'\n",
    "    else:\n",
    "        suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "    return f\"{n}{suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtypes_dict = {'continuous': float, 'categorical': str, 'ordinal': int,\\\n",
    "              'bernoulli': int, 'binomial': int}\n",
    "    \n",
    "dtypes_dict_famd = {'continuous': float, 'categorical': str, 'ordinal': str,\\\n",
    "              'bernoulli': str, 'binomial': str}\n",
    "\n",
    "var_distrib, var_transform_only, le_dict, brace_cols, unique_braces = get_var_metadata(train, train_original, brace_cols, BERNOULLI)\n",
    "p = train.shape[1]\n",
    "dtype = {train.columns[j]: dtypes_dict_famd[var_transform_only[j]] for j in range(p)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GReaT(llm='distilgpt2', batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81912576\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_flat_params(model):\n",
    "    return torch.cat([p.flatten() for p in model.parameters()])\n",
    "initial_params = get_flat_params(model.model).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84559c53064b4287883ec18e602578e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 97.5673, 'train_samples_per_second': 102.493, 'train_steps_per_second': 4.1, 'train_loss': 0.9344205474853515, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "brace_cols_informative = [\"brace type of {} layer\".format(get_ordinal(i+1)) for i in range(len(brace_cols))]\n",
    "layer_cols_informative = [\"{} layer height\".format(get_ordinal(i+1)) for i in range(len(layer_cols))]\n",
    "columns_informative = [\"number of vertical legs\", \"total height\", \"bottom radius\", \"top radius\", \"number of layers\"]\n",
    "\n",
    "columns_informative += brace_cols_informative\n",
    "columns_informative += layer_cols_informative\n",
    "if USE_INFORMATIVE_NAMES:\n",
    "    train_informative = train.copy()\n",
    "    train_informative.columns = columns_informative\n",
    "    print(columns_informative)\n",
    "    model.fit(train_informative)\n",
    "else:\n",
    "    model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of changed parameters: 81208978\n"
     ]
    }
   ],
   "source": [
    "# Get final parameters\n",
    "final_params = get_flat_params(model.model).cpu()\n",
    "\n",
    "# Find the indices where the parameters changed\n",
    "changed_indices = torch.nonzero(initial_params != final_params)\n",
    "changed_params = final_params[changed_indices]\n",
    "\n",
    "print(\"Num of changed parameters:\", len(changed_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 26.25it/s]               \n",
      "15it [00:01,  9.66it/s]                      \n",
      "27it [00:00, 35.38it/s]              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_nb_pseudo_obs = 10\n",
    "# sample 5 times because we loose some from constraints\n",
    "\n",
    "\n",
    "cols = train.columns\n",
    "\n",
    "\n",
    "layer_cols_ = [train.columns.get_loc(col) for col in layer_cols]\n",
    "brace_cols_ = [train.columns.get_loc(col) for col in brace_cols]\n",
    "\n",
    "\n",
    "\n",
    "other_continuous_cols = list(set(np.where(var_distrib == 'continuous')[0]) - set(layer_cols))\n",
    "min_values = train.min()\n",
    "max_values = train.max()\n",
    "\n",
    "ranges = np.array(list(zip(min_values, max_values)))\n",
    "ranges_cont = [ranges[other_continuous_cols], other_continuous_cols]\n",
    "\n",
    "\n",
    "def get_samples(target_nb_pseudo_obs):\n",
    "    y_new_all = []\n",
    "    total_nb_obs_generated = 0\n",
    "    nb_pseudo_obs = 0\n",
    "    nb_points = target_nb_pseudo_obs\n",
    "    while nb_pseudo_obs <= target_nb_pseudo_obs:\n",
    "        y_new = model.sample(n_samples=nb_points,max_length=4000)\n",
    "        if USE_INFORMATIVE_NAMES:\n",
    "            y_new.columns = train.columns\n",
    "        mask = np.apply_along_axis(lambda x: check_constraints(x, var_distrib, le_dict, cols, layer_cols_, brace_cols_, continuous_ranges=ranges_cont), axis=1, arr=y_new)\n",
    "        filtered_rows = y_new[mask]\n",
    "        y_new_all.append(filtered_rows)\n",
    "\n",
    "        total_nb_obs_generated += len(y_new)\n",
    "\n",
    "        nb_pseudo_obs = len(np.concatenate(y_new_all))\n",
    "\n",
    "        nb_points = target_nb_pseudo_obs - nb_pseudo_obs + target_nb_pseudo_obs//10\n",
    "\n",
    "    # Keep target_nb_pseudo_obs pseudo-observations\n",
    "    y_new_all = np.concatenate(y_new_all)\n",
    "    y_new_all = y_new_all[:target_nb_pseudo_obs]\n",
    "    return y_new_all\n",
    "\n",
    "y_new_all = get_samples(target_nb_pseudo_obs)\n",
    "pred = pd.DataFrame(y_new_all, columns = train.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.apply_along_axis(lambda x: check_constraints(x, var_distrib, le_dict, cols, layer_cols_, brace_cols_, continuous_ranges=ranges_cont), axis=1, arr=y_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_post, pred_post_famd = post_process(pred, train.columns, le_dict, discrete_features, brace_cols, dtype, layer_cols_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_output = transform_df_to_json(pred_post)\n",
    "\n",
    "prfx = \"_informative\" if USE_INFORMATIVE_NAMES else \"\"\n",
    "out_name = \"llm(\"+\"_\".join(DATA_NAMES) + \")_\" + str(target_nb_pseudo_obs) + prfx +\".json\"\n",
    "output_filename = res_folder + out_name\n",
    "with open(output_filename, \"w\") as json_file:\n",
    "    json.dump(json_output, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def generate_seeds(target_nb_pseudo_obs, num_seeds=5):\n",
    "#     for seed in range(num_seeds):\n",
    "#         y_new_all = get_samples(target_nb_pseudo_obs)\n",
    "#         pred = pd.DataFrame(y_new_all, columns = train.columns) \n",
    "#         pred_post, pred_post_famd = post_process(pred, train.columns, le_dict, discrete_features, brace_cols, dtype, layer_cols_)\n",
    "#         json_output = transform_df_to_json(pred_post)\n",
    "#         outfolder = res_folder + \"seeds/\" + str(target_nb_pseudo_obs) +\"/\"\n",
    "#         if not os.path.exists(outfolder):\n",
    "#             os.makedirs(outfolder)\n",
    "\n",
    "#         pre = \"llm\"\n",
    "#         if USE_INFORMATIVE_NAMES:\n",
    "#             pre = \"llm_informative\"\n",
    "#         out_name = pre + \"(\"+\"_\".join(DATA_NAMES) + \")_\" + str(target_nb_pseudo_obs) + \"_seed_\" + str(seed) + \".json\"\n",
    "#         output_filename = outfolder + out_name\n",
    "#         with open(output_filename, \"w\") as json_file:\n",
    "#             json.dump(json_output, json_file, indent=4)\n",
    "\n",
    "# checkpoints = [1500, 2000]\n",
    "# for checkpoint in checkpoints:\n",
    "#     generate_seeds(checkpoint, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate_plots([\"real\", \"synthetic (GA)\", \"synthetic\"], train_original, pred_post, None, None, var_distrib, le_dict, brace_cols, unique_braces, res_folder)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:06, 19.50it/s]                        \n",
      "58it [00:04, 12.48it/s]                        \n",
      "100%|██████████| 22/22 [00:01<00:00, 14.07it/s]\n",
      "18it [00:00, 23.19it/s]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [00:06, 17.81it/s]                        \n",
      "62it [00:03, 19.35it/s]                        \n",
      "28it [00:01, 18.19it/s]                        \n",
      "18it [00:01, 11.51it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:05, 22.13it/s]                        \n",
      "76it [00:03, 24.53it/s]                        \n",
      "36it [00:02, 15.25it/s]                        \n",
      "17it [00:01, 10.85it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 12.636540730794271\n",
      "Processing target_nb_pseudo_obs = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [00:32, 15.64it/s]                         \n",
      "100%|██████████| 279/279 [00:14<00:00, 18.78it/s]\n",
      "129it [00:06, 18.44it/s]                         \n",
      "59it [00:03, 19.09it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [00:29, 17.21it/s]                         \n",
      "278it [00:15, 18.03it/s]                         \n",
      "126it [00:07, 17.89it/s]                         \n",
      "63it [00:03, 16.01it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "502it [00:29, 17.14it/s]                         \n",
      "280it [00:16, 16.94it/s]                         \n",
      "121it [00:08, 13.90it/s]                         \n",
      "63it [00:03, 19.88it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 56.99722329775492\n",
      "Processing target_nb_pseudo_obs = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1006it [00:47, 21.27it/s]                         \n",
      "548it [00:29, 18.50it/s]                         \n",
      "265it [00:17, 15.27it/s]                         \n",
      "134it [00:07, 18.54it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1007it [00:58, 17.10it/s]                         \n",
      "560it [00:30, 18.42it/s]                         \n",
      "261it [00:17, 15.02it/s]                         \n",
      "130it [00:06, 19.61it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target_nb_pseudo_obs = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1006it [00:55, 18.03it/s]                         \n",
      "562it [00:35, 15.76it/s]                         \n",
      "268it [00:16, 16.14it/s]                         \n",
      "100%|██████████| 108/108 [00:07<00:00, 14.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 110.56907526652019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store results\n",
    "target_values = [100, 500, 1000]\n",
    "num_iterations = 3  # Number of times to run the function for each target value\n",
    "\n",
    "for target_nb_pseudo_obs in target_values:\n",
    "    times_for_target = []\n",
    "    execution_times = []\n",
    "    for _ in range(num_iterations):\n",
    "        print(f\"Processing target_nb_pseudo_obs = {target_nb_pseudo_obs}\")\n",
    "        start_time = time.time()\n",
    "        y_new_all = get_samples(target_nb_pseudo_obs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append(execution_time)\n",
    "    print(target_nb_pseudo_obs, np.mean(execution_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a file\n",
    "with open(res_folder+'execution_times_new.txt', 'w') as f:\n",
    "    for target_nb_pseudo_obs, mean_time in zip(target_values, execution_times):\n",
    "        f.write(f'Target obs: {target_nb_pseudo_obs}, Mean Time: {mean_time:.4f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
