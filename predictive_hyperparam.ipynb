{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ON CPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils_methods import encode, get_var_metadata, post_process, transform_df_to_json, generate_plots, check_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compression', 'Cost', 'combined', 'torque', 'wave', 'tipover']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_folder = \"results/evaluated/\"\n",
    "with open(data_folder + \"real/real_structures_evaluated.json\") as f:\n",
    "    real = json.load(f)\n",
    "with open(\"results/real/real_structures.json\") as f:\n",
    "    real_not_eval = json.load(f)\n",
    "\n",
    "irrelevant = set(real_not_eval[0].keys())\n",
    "col_set = set(real[0].keys())\n",
    "objectives_to_extract = ['compression', 'Cost', 'combined', 'torque', 'wave','tipover']\n",
    "print(objectives_to_extract)\n",
    "\n",
    "modelll = \"vae\"\n",
    "\n",
    "ldr = os.listdir(\"results/evaluated/hyperparam/\"+modelll)\n",
    "\n",
    "ldr.sort()\n",
    "DATA_ARRAY = [real]\n",
    "DATA_NAMES = [\"real\"]\n",
    "for dr in ldr:\n",
    "    with open(\"results/evaluated/hyperparam/{}/\".format(modelll)+dr) as f:\n",
    "        dt = json.load(f)\n",
    "    DATA_ARRAY.append(dt)\n",
    "    DATA_NAMES.append(dr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brace_dict = {    \n",
    "    \"NONE\": 0,\n",
    "    \"H\": 1,\n",
    "    \"Z\": 2,\n",
    "    \"IZ\": 3,\n",
    "    \"ZH\": 4,\n",
    "    \"IZH\": 5,\n",
    "    \"K\": 6,\n",
    "    \"X\": 7,\n",
    "    \"XH\": 8,\n",
    "    \"nan\": 9,\n",
    "}\n",
    "\n",
    "brace_dict_inv = dict(zip(brace_dict.values(), brace_dict.keys()))\n",
    "\n",
    "N_BRACES = len(brace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = max(d.get('n_layers', 0) for sublist in DATA_ARRAY for d in sublist)\n",
    "transformed_columns = [\"legs\", \"total_height\", \"radius_bottom\", \"radius_top\", \"n_layers\"]\n",
    "brace_cols = [\"brace\" + str(i) for i in range(max_layers-1)] \n",
    "transformed_columns += brace_cols\n",
    "transformed_columns += [\"layer_height\" + str(i) for i in range(max_layers-2)]\n",
    "\n",
    "encoded = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "\n",
    "y_columns_to_extract = objectives_to_extract\n",
    "\n",
    "for dataset, name in zip(DATA_ARRAY, DATA_NAMES):\n",
    "\n",
    "    encoding = [encode(d, max_layers, brace_dict, N_BRACES, one_hot=False, native=True, normalize_layer_heights=True) for d in dataset]\n",
    "    extracted_data = [{col: entry[col] for col in y_columns_to_extract} for entry in dataset]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    y = pd.DataFrame(extracted_data)\n",
    "    encoded.append(encoding)\n",
    "    Y.append(y)\n",
    "\n",
    "    \n",
    "X = []\n",
    "for count, encoding in enumerate(encoded):\n",
    "    df_ = pd.DataFrame(encoding, columns=transformed_columns)\n",
    "    # df_[\"label\"] = [DATA_NAMES[count]]*len(df_)\n",
    "    X.append(df_.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brace0': {'X': 0, 'K': 1}, 'brace1': {'X': 0, 'nan': 1}, 'brace2': {'X': 0, 'nan': 1, 'NONE': 2}, 'brace3': {'X': 0, 'nan': 1, 'NONE': 2}, 'brace4': {'X': 0, 'nan': 1}}\n"
     ]
    }
   ],
   "source": [
    "columns_one_hot = brace_cols\n",
    "columns_standard = list(set(transformed_columns) - set(brace_cols))\n",
    "# augmented = pd.concatenate(X) \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_one_hot, columns_standard, unique_categories, columns):\n",
    "        self.columns_one_hot = columns_one_hot\n",
    "        self.columns_standard = columns_standard\n",
    "        self.columns = columns\n",
    "        self.unique_categories = unique_categories\n",
    "        self.encoders = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for i in range(X.shape[-1]):\n",
    "            name = self.columns[i]\n",
    "            if name in columns_standard:\n",
    "                encoder = StandardScaler()\n",
    "                encoder.fit(X.iloc[:,i:i+1])\n",
    "            self.encoders.append(encoder)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        transformed = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            name = self.columns[i]\n",
    "            if name in columns_standard:\n",
    "                tr = encoder.transform(X.iloc[:, i:i+1])\n",
    "                transformed.append(tr)\n",
    "            else:\n",
    "                mapping = self.unique_categories[name]\n",
    "                tr = X[name].map(mapping).values.reshape(-1,1)\n",
    "                transformed.append(tr)\n",
    "                \n",
    "        return np.concatenate(transformed, axis=-1)\n",
    "\n",
    "def get_unique_all(X, columns_one_hot):\n",
    "\n",
    "    all_categories = {}\n",
    "    for column in columns_one_hot:\n",
    "        unique_categories = set()\n",
    "        for df in X:\n",
    "            unique_categories.update(df[column].unique())\n",
    "        all_categories[column] = mapping = {val: idx for idx, val in enumerate(list(unique_categories))} \n",
    "    return all_categories\n",
    "\n",
    "unique_categories = get_unique_all(X, columns_one_hot)\n",
    "print(unique_categories)\n",
    "enc_columntransfo = CustomTransformer(columns_one_hot, columns_standard, unique_categories, X[0].columns)\n",
    "cty = MinMaxScaler()\n",
    "\n",
    "X_real_transformed = pd.DataFrame(enc_columntransfo.fit_transform(X[0]))\n",
    "y_real_transformed = cty.fit_transform(Y[0])\n",
    "# y_real_transformed = np.array(Y[0])\n",
    "\n",
    "synthetic_Xs = []\n",
    "synthetic_ys = []\n",
    "\n",
    "for i in range(1, len(X)): \n",
    "    synthetic_Xs.append(enc_columntransfo.transform(X[i]))\n",
    "    synthetic_ys.append(cty.transform(Y[i]))\n",
    "    # synthetic_ys.append(np.array(Y[i]))\n",
    "\n",
    "# y_inv = cty.inverse_transform(y_real_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(names_all, problem, all_metrics_mean, all_metrics_std):\n",
    "    all_cols = str(len(names_all)**2 +1)\n",
    "    latex_table = \"\\\\begin{table}\\n\"\n",
    "    latex_table += \"\\\\centering\\n\"\n",
    "    latex_table += \"\\\\scalebox{0.70}{\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{|c|*{\" + all_cols + \"}{c|}}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    \n",
    "    latex_table += \"Train data & \\\\multicolumn{\" + str(len(names_all)) + \"}{c|}{Real} & \"\n",
    "    latex_table += \"\\\\multicolumn{\" + str(len(names_all)) + \"}{c|}{Synthetic} & \"\n",
    "    latex_table += \"\\\\multicolumn{\" + str(len(names_all)) + \"}{c|}{Augmented} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "    \n",
    "    latex_table += \"Test data & \" + \" & \".join([name for _ in range(len(names_all)) for name in names_all]) + \" \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "    \n",
    "    # for name in names_all:\n",
    "    #     for _ in range(len(problem[\"metric_names\"])):\n",
    "    #         latex_table += f\"& {name} \"\n",
    "    # latex_table += \"\\\\\\\\\\n\"\n",
    "    # latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "    \n",
    "    for metric_row in range(len(problem[\"metric_names\"])):\n",
    "        metric_name = problem[\"metric_names\"][metric_row]\n",
    "        latex_table += f\"{metric_name} \"\n",
    "        for metric_col in range(len(names_all)):\n",
    "            for name_row in range(len(names_all)):\n",
    "                    # print(all_metrics_mean[metric_row])\n",
    "                    # print(all_metrics_mean[metric_row][name_row])\n",
    "                    avg_metric = all_metrics_mean[metric_row][name_row][metric_col]\n",
    "                    std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "                    latex_table += f\"& {avg_metric:.3f} ({std_metric:.3f})\"\n",
    "        latex_table += \" \\\\\\\\\\n\"\n",
    "        if metric_row < len(problem[\"metric_names\"]) - 1:\n",
    "            latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "        else:\n",
    "            latex_table += \"\\\\hline\\n\"\n",
    "    \n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    latex_table += \"}\\n\"\n",
    "    latex_table += \"\\\\caption{Comparison}\\n\"\n",
    "    latex_table += \"\\\\label{tab:eval}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def compute_metrics(y_test, y_pred, problem, name_tr, name_t):\n",
    "    return [m(y_test, y_pred) for m in problem[\"metrics\"]]\n",
    "\n",
    "def train_eval(X_train, y_train, X_test, y_test, problem, name_tr, name_t):\n",
    "    model = problem[\"model\"](**problem[\"args\"])\n",
    "    try:\n",
    "        model.predict(X_test)\n",
    "        print(name_tr, name_t, \"Model is already fitted!\")\n",
    "        exit(1)\n",
    "    except:\n",
    "        pass\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = compute_metrics(y_test, y_pred, problem, name_tr, name_t)\n",
    "    return metrics, y_pred\n",
    "\n",
    "def make_problem(X_real, y_real, X_s, y_s, problem, names_all, print_percentages=False):\n",
    "    if \"regression\" in problem.keys():\n",
    "        X_r = np.array(X_real)\n",
    "        y_r = np.array(y_real[:,problem[\"regression\"]])\n",
    "        ll = []\n",
    "        for y in y_s:\n",
    "            ll.append(np.array(y[:,problem[\"regression\"]]))\n",
    "    else:\n",
    "        X_r = np.array(X_real)\n",
    "        threshold = problem[\"threshold_metric\"](y_real[:,problem[\"classification\"]])\n",
    "        y_r_binary = (y_real[:,problem[\"classification\"]] > threshold).astype(int)\n",
    "        y_r = np.array(y_r_binary)\n",
    "        if print_percentages:\n",
    "            print('\\n'.join(f'{names_all[0]} Class {c}: {count/len(y_r)*100:.2f}%' for c, count in enumerate(np.bincount(y_r))))\n",
    "\n",
    "        ll = []\n",
    "        for y, n in zip(y_s, names_all[1:]):\n",
    "            threshold = problem[\"threshold_metric\"](y[:,problem[\"classification\"]])\n",
    "            y_binary = (y[:,problem[\"classification\"]] > threshold).astype(int)\n",
    "            y_binary = np.array(y_binary)\n",
    "            if print_percentages:\n",
    "                print('\\n'.join(f'{n} Class {c}: {count/len(y_binary)*100:.2f}%' for c, count in enumerate(np.bincount(y_binary))))\n",
    "\n",
    "            ll.append(y_binary)\n",
    "    return X_r, y_r, [np.array(x) for x in X_s], ll\n",
    "\n",
    "def run_experiments(X_real, y_real, synthetic_Xs, synthetic_ys, problems, names_all, output_names, bw=0.5,\n",
    "                     plot=False, augment=False, num_repeats = 1, num_folds = 2):\n",
    "    average_problems = []\n",
    "    std_problems = []\n",
    "    for problem in problems:\n",
    "        print(\"Model\", problem[\"model_name\"])    \n",
    "\n",
    "        X_real_, y_real_, synthetic_Xs_, synthetic_ys_ = make_problem(X_real, y_real, synthetic_Xs, synthetic_ys, problem, names_all)\n",
    "        # Assuming you have X and y defined\n",
    "\n",
    "        # Create an MLP regression model\n",
    "\n",
    "        # Set up repeated cross-validation\n",
    "\n",
    "        rkf = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeats, random_state=42)\n",
    "        all_metrics_mean = []\n",
    "        all_metrics_std = []\n",
    "        # for train_i in range(len(names_in)):\n",
    "        #     metrics_all = [[]]*len(names_in)\n",
    "        metrics_all = []\n",
    "        for i, (train_index, test_index) in enumerate(rkf.split(X_real_)):    \n",
    "            X_train_real, X_test_real = X_real_[train_index], X_real_[test_index]\n",
    "            y_train_real, y_test_real = y_real_[train_index], y_real_[test_index]\n",
    "\n",
    "\n",
    "            X_trains = [X_train_real]\n",
    "            y_trains = [y_train_real]\n",
    "            X_tests = [X_test_real]\n",
    "            y_tests = [y_test_real]\n",
    "\n",
    "            metrics_split = []\n",
    "            k = 0 \n",
    "            if not augment:\n",
    "                for X_synthetic, y_synthetic in zip(synthetic_Xs_, synthetic_ys_):\n",
    "                    X_trains.append(X_synthetic)\n",
    "                    y_trains.append(y_synthetic)\n",
    "            else:\n",
    "                for X_synthetic, y_synthetic in zip(synthetic_Xs_, synthetic_ys_):\n",
    "                    X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n",
    "\n",
    "                    X_trains.append(X_train_synthetic)\n",
    "                    y_trains.append(y_train_synthetic)\n",
    "\n",
    "                    X_tests.append(X_test_synthetic)\n",
    "                    y_tests.append(y_test_synthetic)\n",
    "\n",
    "                    X_train_augmented = np.append(X_train_real, X_train_synthetic, axis=0)   \n",
    "                    y_train_augmented = np.append(y_train_real, y_train_synthetic, axis=0)   \n",
    "                    X_test_augmented = np.append(X_test_real, X_test_synthetic, axis=0)   \n",
    "                    y_test_augmented = np.append(y_test_real, y_test_synthetic, axis=0)  \n",
    "\n",
    "                    X_trains.append(X_train_augmented)\n",
    "                    y_trains.append(y_train_augmented)\n",
    "                    X_tests.append(X_test_augmented)\n",
    "                    y_tests.append(y_test_augmented)\n",
    "\n",
    "\n",
    "            for X_tr, y_tr, name_tr in zip(X_trains, y_trains, names_all):\n",
    "                setup_metrics = []\n",
    "                preds = []  \n",
    "                for X_t, y_t, name_t in zip(X_tests, y_tests, names_all):\n",
    "                    results, pred = train_eval(X_tr, y_tr, X_t, y_t, problem, name_tr, name_t)\n",
    "                    setup_metrics.append(results)\n",
    "                    preds.append(pred)\n",
    "                metrics_split.append(setup_metrics)\n",
    "                \n",
    "            # if plot:\n",
    "            #     fig, ax = plt.subplots(figsize=(20, 5), nrows=1, ncols=y_t.shape[-1])\n",
    "            #     for i in range(y_t.shape[-1]):\n",
    "            #         g = sns.kdeplot(y_t[:, i], label=\"Ground Truth\", linestyle=\"dashed\", ax=ax[i], bw_adjust=bw ).set(title=output_names[i] + \" (test on \" + name_t + \")\")\n",
    "            #         for pred, na in zip(preds, names_all):\n",
    "            #             mse = mean_squared_error(pred[:, i], y_t[:, i])\n",
    "            #             g = sns.kdeplot(pred[:, i], label=\"pred (trained \" + na + \") \" + \"\\nMSE={:.3f}\".format(mse), ax=ax[i], bw_adjust=bw)\n",
    "            #         ax[i].legend(loc='center', bbox_to_anchor=(0.4, -0.3))\n",
    "            #     annot = \"\"\n",
    "            #     for pred, na, result in zip(preds, names_all, setup_metrics):\n",
    "            #         mse_total = mean_squared_error(pred, y_t)\n",
    "            #         annot += \"Total MSE (trained \" + na + \")={:.3f}\\n\".format(mse_total)\n",
    "            #     plt.tight_layout()\n",
    "            #     plt.annotate(annot, (0.4, 0), xycoords='figure fraction')\n",
    "            #     if plot and k==0:\n",
    "            #         plt.show()\n",
    "            # metrics_split = np.transpose(np.array(metrics_split), (1, 0, 2))\n",
    "            metrics_all.append(metrics_split)\n",
    "            # Calculate the overall average scores across all repeats\n",
    "        metrics_all = np.array(metrics_all)    \n",
    "        average_metrics_all = np.mean(metrics_all, axis=0)\n",
    "        std_metrics_all = np.std(metrics_all, axis=0)\n",
    "        average_problems.append(average_metrics_all)\n",
    "        std_problems.append(std_metrics_all)\n",
    "    return np.array(average_problems), np.array(std_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import copy\n",
    "\n",
    "all_columns = list(range(y_real_transformed.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "#random forest, boosting, xgb (forest), snn (sequential nn)\n",
    "\n",
    "problem_regression = {\"regression\":all_columns, \n",
    "                      \"metrics\":[mean_squared_error, r2_score],\n",
    "                      \"metric_names\":[\"MSE\", \"R2\"]}\n",
    "                      \n",
    "# models = [MultiOutputRegressor(LGBMRegressor(random_state=42)), DecisionTreeRegressor(random_state=42), RandomForestRegressor(random_state=42)]\n",
    "models = [CatBoostRegressor, DecisionTreeRegressor, RandomForestRegressor]\n",
    "args = [{\"random_state\":42, \"loss_function\":\"MultiRMSE\", \"verbose\":False, \"iterations\":100, \"learning_rate\":0.01}, {\"random_state\":42}, {\"random_state\":42}]\n",
    "\n",
    "model_names = [\"CBR\", \"DT\", \"RF\"]\n",
    "problems_regression = []\n",
    "for model, name, arg in zip(models, model_names, args):\n",
    "    problem = problem_regression.copy()\n",
    "    problem[\"model\"] = model\n",
    "    problem[\"model_name\"] = name\n",
    "    problem[\"args\"] = arg\n",
    "    problems_regression.append(problem)\n",
    "\n",
    "problem_classification = {\"classification\":0, \"threshold_metric\":np.median,\n",
    "                          \"metrics\":[accuracy_score, f1_score, roc_auc_score],\n",
    "                          \"metric_names\":[\"Accuracy\", \"F1\", \"ROC-AUC\"]}\n",
    "                      \n",
    "models_classification = [LogisticRegression(random_state=42, max_iter=1000), DecisionTreeClassifier(random_state=42), RandomForestClassifier(random_state=42)]\n",
    "model_names_classification = [\"LR\", \"DT\", \"RF\"]\n",
    "problems_classification = []\n",
    "for model, name in zip(models_classification, model_names_classification):\n",
    "    problem = problem_classification.copy()\n",
    "    problem[\"model\"] = copy.deepcopy(model)\n",
    "    problem[\"model_name\"] = name\n",
    "    problems_classification.append(problem)\n",
    "\n",
    "# problem_classification = {\"classification\":0, \"threshold_metric\":np.median,\n",
    "#                           \"metrics\":[accuracy_score,  precision_score, recall_score, f1_score, roc_auc_score],\n",
    "#                           \"metric_names\":[\"Accuracy\", \"P\", \"R\", \"F1\", \"ROC-AUC\"],\n",
    "#                           \"model\":MLPClassifier(hidden_layer_sizes=(50), max_iter=1000, random_state=42)}\n",
    "\n",
    "# problem_classification_forest = {\"classification\":0, \"threshold_metric\":np.median,\n",
    "#                           \"metrics\":[accuracy_score],\n",
    "#                           \"metric_names\":[\"Accuracy\"],\n",
    "#                           \"model\":RandomForestClassifier(random_state=42)}\n",
    "\n",
    "# problem_regression_forest = {\"regression\":all_columns, \n",
    "#                       \"metrics\":[mean_squared_error, mean_absolute_error, r2_score],\n",
    "#                       \"metric_names\":[\"MSE\", \"MAE\", \"R2\"],\n",
    "#                       \"model\":RandomForestRegressor(random_state=42)}\n",
    "\n",
    "# train_eval(MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42), np.array(X_real_transformed), np.array(y_real_transformed), np.array(X_synthetic_transformed), np.array(y_synthetic_transformed))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# names_in = DATA_NAMES\n",
    "# augmented = [\"\"]\n",
    "# augmented += [names_in[0] + \" + \" + x for x in names_in[1:]]\n",
    "# names_all = [names_in[0]]\n",
    "# for i in range(1, len(names_in)):\n",
    "#     names_all.append(names_in[i])\n",
    "#     names_all.append(augmented[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "Model CBR\n",
      "Model DT\n",
      "Model RF\n",
      "[3, 4, 5]\n",
      "Model CBR\n",
      "Model DT\n",
      "Model RF\n",
      "[6, 7, 8]\n",
      "Model CBR\n",
      "Model DT\n",
      "Model RF\n",
      "[9, 10, 11]\n",
      "Model CBR\n",
      "Model DT\n",
      "Model RF\n"
     ]
    }
   ],
   "source": [
    "names_all = DATA_NAMES\n",
    "np_names = np.array(DATA_NAMES[1:])\n",
    "\n",
    "new_names_all = []\n",
    "new_avg_ = []\n",
    "new_std_ = []\n",
    "for i in range(0, len(names_all)-3, 3):\n",
    "    idx = [i,i+1,i+2]\n",
    "    xs = []\n",
    "    ys = []\n",
    "    print(idx)\n",
    "    for id in idx:\n",
    "        xs.append(synthetic_Xs[id])\n",
    "        ys.append(synthetic_ys[id])\n",
    "    new_names = np_names[idx]\n",
    "    name_no_seed = new_names[0].split(\"_1000\")[0]\n",
    "    new_names_all.append(name_no_seed)\n",
    "    problems_used = problems_regression\n",
    "    average, std = run_experiments(X_real_transformed, y_real_transformed, xs, ys, problems_used, new_names, Y[0].columns, num_folds=3, num_repeats=5)\n",
    "    average_ = np.mean(average, axis=1)\n",
    "    std_ = np.mean(std, axis=1)\n",
    "    new_avg_.append(average_)\n",
    "    new_std_.append(std_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_avg = np.array(new_avg_)\n",
    "new_std = np.array(new_std_)\n",
    "new_avg = np.transpose(new_avg, (1, 0, 2, 3))\n",
    "new_std = np.transpose(new_std, (1, 0, 2, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(new_avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBR encdims1282023evaluated.json-Real MSE: 0.017 (0.007)\n",
      "CBR encdims1282023evaluated.json-Real R2: 0.346 (0.087)\n",
      "CBR encdims322023evaluated.json-Real MSE: 0.019 (0.007)\n",
      "CBR encdims322023evaluated.json-Real R2: 0.231 (0.089)\n",
      "CBR encdims642023evaluated.json-Real MSE: 0.016 (0.006)\n",
      "CBR encdims642023evaluated.json-Real R2: 0.405 (0.080)\n",
      "CBR encdims82023evaluated.json-Real MSE: 0.022 (0.007)\n",
      "CBR encdims82023evaluated.json-Real R2: 0.124 (0.074)\n",
      "DT encdims1282023evaluated.json-Real MSE: 0.010 (0.004)\n",
      "DT encdims1282023evaluated.json-Real R2: 0.544 (0.194)\n",
      "DT encdims322023evaluated.json-Real MSE: 0.014 (0.006)\n",
      "DT encdims322023evaluated.json-Real R2: 0.372 (0.242)\n",
      "DT encdims642023evaluated.json-Real MSE: 0.009 (0.004)\n",
      "DT encdims642023evaluated.json-Real R2: 0.547 (0.235)\n",
      "DT encdims82023evaluated.json-Real MSE: 0.019 (0.006)\n",
      "DT encdims82023evaluated.json-Real R2: 0.144 (0.237)\n",
      "RF encdims1282023evaluated.json-Real MSE: 0.007 (0.003)\n",
      "RF encdims1282023evaluated.json-Real R2: 0.737 (0.077)\n",
      "RF encdims322023evaluated.json-Real MSE: 0.012 (0.005)\n",
      "RF encdims322023evaluated.json-Real R2: 0.499 (0.128)\n",
      "RF encdims642023evaluated.json-Real MSE: 0.006 (0.003)\n",
      "RF encdims642023evaluated.json-Real R2: 0.698 (0.102)\n",
      "RF encdims82023evaluated.json-Real MSE: 0.017 (0.006)\n",
      "RF encdims82023evaluated.json-Real R2: 0.305 (0.120)\n"
     ]
    }
   ],
   "source": [
    "names_train = new_names_all\n",
    "names_train = [s.replace('_', '').replace('[', '').replace(']', '') for s in new_names_all]\n",
    "\n",
    "# names_test = names_all\n",
    "names_test = [\"Real\"]\n",
    "for model_name_i in range(len(model_names)):\n",
    "    model_name = model_names[model_name_i]\n",
    "    for train_i in range(len(names_train)):\n",
    "        name_train = names_train[train_i]\n",
    "        for test_i in range(len(names_test)):\n",
    "            name_test = names_test[test_i]\n",
    "            for j in range(len(problems_used[0][\"metric_names\"])):\n",
    "                print(f\"{model_name} {name_train}-{name_test} {problems_used[0]['metric_names'][j]}: {new_avg[model_name_i][train_i][test_i][j]:.3f} ({new_std[model_name_i][train_i][test_i][j]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table1(all_metrics_mean, all_metrics_std, names_train, names_test, problems, test_data=False):\n",
    "    all_rows = str(len(names_all)**2 +1)\n",
    "    if test_data:\n",
    "        all_cols = str(len(problems[0][\"metric_names\"]) + 3)\n",
    "    else:\n",
    "        all_cols = str(len(problems[0][\"metric_names\"]) + 2)\n",
    "    latex_table = \"\\\\begin{table}\\n\"\n",
    "    latex_table += \"\\\\centering\\n\"\n",
    "    # latex_table += \"\\\\scalebox{0.70}{\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{|c|*{\" + all_cols + \"}{c|}}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    if test_data:\n",
    "        latex_table += \"Model & Train data & Test data & \" + \" & \".join(problems[0][\"metric_names\"]) + \" \\\\\\\\\\n\"\n",
    "    else:\n",
    "        latex_table += \"Model & Train data & \" + \" & \".join(problems[0][\"metric_names\"]) + \" \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\"\n",
    "    for problem_i in range(len(problems)):\n",
    "        latex_table += \"\\\\multirow{\" + str(len(problems)) + \"}{*}{\" + problems[problem_i][\"model_name\"] + \"}\"\n",
    "\n",
    "        for i in range(len(names_train)):\n",
    "            train_name = names_train[i]\n",
    "            latex_table += \" & \" + \"\\\\multirow{\" + str(len(names_test)) + \"}{*}{\" + train_name + \"}\"\n",
    "            # avg_metric = all_metrics_mean[metric_row][name_row][metric_col]\n",
    "            # std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "            # latex_table += f\"& {avg_metric:.3f} ({std_metric:.3f})\"\n",
    "            for j in range(len(names_test)):\n",
    "                test_name = names_test[j]\n",
    "                avg_metric = all_metrics_mean[problem_i][i][j]\n",
    "                std_metric = all_metrics_std[problem_i][i][j]\n",
    "                # std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "                numbers = \" & \".join(map(lambda xy: \"{:.3f} ({:.3f})\".format(100*xy[0], 100*xy[1]), zip(avg_metric, std_metric)))\n",
    "\n",
    "                if test_data:\n",
    "                    latex_table += \" & \" + test_name + \" & \" +  numbers + \" \\\\\\\\\\n\"\n",
    "                else:\n",
    "                    latex_table += \" & \" + numbers + \" \\\\\\\\\\n\"\n",
    "\n",
    "                latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "        latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    # latex_table += \"}\\n\"\n",
    "    latex_table += \"\\\\caption{Comparison}\\n\"\n",
    "    latex_table += \"\\\\label{tab:eval}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{|c|*{4}{c|}}\n",
      "\\hline\n",
      "Model & Train data & MSE & R2 \\\\\n",
      "\\hline\\multirow{3}{*}{CBR} & \\multirow{1}{*}{encdims1282023evaluated.json} & 1.711 (0.668) & 34.592 (8.651) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims322023evaluated.json} & 1.915 (0.686) & 23.082 (8.863) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims642023evaluated.json} & 1.626 (0.618) & 40.515 (7.987) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims82023evaluated.json} & 2.190 (0.736) & 12.420 (7.380) \\\\\n",
      "\\cline{2-4}\n",
      "\\hline\n",
      "\\multirow{3}{*}{DT} & \\multirow{1}{*}{encdims1282023evaluated.json} & 1.002 (0.392) & 54.383 (19.375) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims322023evaluated.json} & 1.419 (0.554) & 37.220 (24.163) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims642023evaluated.json} & 0.893 (0.381) & 54.685 (23.509) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims82023evaluated.json} & 1.904 (0.579) & 14.356 (23.715) \\\\\n",
      "\\cline{2-4}\n",
      "\\hline\n",
      "\\multirow{3}{*}{RF} & \\multirow{1}{*}{encdims1282023evaluated.json} & 0.693 (0.337) & 73.690 (7.712) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims322023evaluated.json} & 1.239 (0.511) & 49.940 (12.840) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims642023evaluated.json} & 0.622 (0.306) & 69.822 (10.213) \\\\\n",
      "\\cline{2-4}\n",
      " & \\multirow{1}{*}{encdims82023evaluated.json} & 1.676 (0.550) & 30.509 (11.953) \\\\\n",
      "\\cline{2-4}\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Comparison}\n",
      "\\label{tab:eval}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "latex_table = generate_latex_table1(new_avg, new_std, names_train, names_test, problems_used)\n",
    "print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
